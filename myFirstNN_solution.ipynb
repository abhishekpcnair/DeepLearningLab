{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mancinimassimiliano/DeepLearningLab/blob/master/myFirstNN_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "D70JB83_IUwE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1823
        },
        "outputId": "28f8b599-dd3d-44bb-deae-9f7f19687c6d"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "\n",
        "DEVICE='cuda:0'\n",
        "LR=0.1\n",
        "WEIGHT_DECAY = 0.000001 \n",
        "MOMENTUM = 0.9\n",
        "BATCH_SIZE = 256\n",
        "TEST_BATCH_SIZE = 500\n",
        "EPOCHS = 20\n",
        "\n",
        "INPUT_DIM = 784\n",
        "HIDDEN_DIM = 100\n",
        "OUTPUT_DIM = 10\n",
        "\n",
        "\n",
        "class MyFirstNetwork(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "    super(MyFirstNetwork, self).__init__()\n",
        "    self.input_to_hidden = nn.Linear(input_dim, hidden_dim)\n",
        "    self.activation = nn.ReLU()\n",
        "    self.hidden_to_output = nn.Linear(hidden_dim, output_dim)\n",
        "    \n",
        "    self.input_to_hidden.weight.data.normal_(0,0.1)\n",
        "    self.input_to_hidden.bias.data.fill_(0.0)\n",
        "    self.hidden_to_output.weight.data.normal_(0,0.1)\n",
        "    self.hidden_to_output.bias.data.fill_(0.0)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(x.shape[0],-1)\n",
        "    x = self.input_to_hidden(x)\n",
        "    x = self.activation(x)\n",
        "    x = self.hidden_to_output(x)\n",
        "    return x\n",
        "  \n",
        "\n",
        "def get_data(): \n",
        "  transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
        "\n",
        "  full_training_data = torchvision.datasets.MNIST('.', train=True, download=True, transform=transform) \n",
        "  test_data = torchvision.datasets.MNIST('.', train=False, download=True, transform=transform)\n",
        "  \n",
        "\n",
        "\n",
        "  num_samples = len(full_training_data)\n",
        "  training_samples = int(num_samples*0.5+1)\n",
        "  validation_samples = num_samples - training_samples\n",
        "\n",
        "  training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(training_data, batch_size = BATCH_SIZE, shuffle=True)\n",
        "  val_loader = torch.utils.data.DataLoader(validation_data, batch_size = TEST_BATCH_SIZE, shuffle=False)\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, batch_size = TEST_BATCH_SIZE, shuffle=False)\n",
        "  \n",
        "  return train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "def get_optimizer(net, lr, wd, momentum):\n",
        "  optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n",
        "  return optimizer\n",
        "\n",
        "\n",
        "def get_cost_function():\n",
        "  return nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "def test(net, data_loader, cost_function):\n",
        "  net.eval()\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      # Load data into GPU\n",
        "      inputs = inputs.to(DEVICE)\n",
        "      targets = targets.to(DEVICE)\n",
        "        \n",
        "      # Forward pass\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      # Apply the loss\n",
        "      loss = cost_function(outputs,targets)\n",
        "\n",
        "      # Better print something, no?\n",
        "      samples+=inputs.shape[0]\n",
        "      cumulative_loss += loss.item()\n",
        "      _, predicted = outputs.max(1)\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(net,data_loader,optimizer,cost_function):\n",
        "  net.train()\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "    # Load data into GPU\n",
        "    inputs = inputs.to(DEVICE)\n",
        "    targets = targets.to(DEVICE)\n",
        "      \n",
        "    # Forward pass\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    # Apply the loss\n",
        "    loss = cost_function(outputs,targets)\n",
        "\n",
        "    # Reset the optimizer\n",
        "    optimizer.zero_grad()\n",
        "      \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "      \n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # Better print something, no?\n",
        "    samples+=inputs.shape[0]\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100\n",
        "\n",
        "\n",
        "def main():\n",
        "  train_loader, val_loader, test_loader = get_data()\n",
        "  net = MyFirstNetwork(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM).to(DEVICE)\n",
        "  optimizer = get_optimizer(net, LR, WEIGHT_DECAY, MOMENTUM)\n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  print('Before training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  for e in range(EPOCHS):\n",
        "    train_loss, train_accuracy = train(net, train_loader, optimizer, cost_function)\n",
        "    val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "main()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/9912422 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:00, 26975597.39it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 408587.28it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 6735366.45it/s]                           \n",
            "8192it [00:00, 168031.43it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n",
            "Before training:\n",
            "\t Training loss 0.00954, Training accuracy 11.98\n",
            "\t Validation loss 0.00486, Validation accuracy 11.86\n",
            "\t Test loss 0.00489, Test accuracy 11.27\n",
            "-----------------------------------------------------\n",
            "Epoch: 1\n",
            "\t Training loss 0.00195, Training accuracy 84.96\n",
            "\t Validation loss 0.00056, Validation accuracy 91.82\n",
            "-----------------------------------------------------\n",
            "Epoch: 2\n",
            "\t Training loss 0.00084, Training accuracy 93.63\n",
            "\t Validation loss 0.00039, Validation accuracy 94.36\n",
            "-----------------------------------------------------\n",
            "Epoch: 3\n",
            "\t Training loss 0.00060, Training accuracy 95.54\n",
            "\t Validation loss 0.00032, Validation accuracy 95.26\n",
            "-----------------------------------------------------\n",
            "Epoch: 4\n",
            "\t Training loss 0.00046, Training accuracy 96.56\n",
            "\t Validation loss 0.00030, Validation accuracy 95.54\n",
            "-----------------------------------------------------\n",
            "Epoch: 5\n",
            "\t Training loss 0.00038, Training accuracy 97.07\n",
            "\t Validation loss 0.00027, Validation accuracy 96.00\n",
            "-----------------------------------------------------\n",
            "Epoch: 6\n",
            "\t Training loss 0.00031, Training accuracy 97.62\n",
            "\t Validation loss 0.00026, Validation accuracy 96.14\n",
            "-----------------------------------------------------\n",
            "Epoch: 7\n",
            "\t Training loss 0.00027, Training accuracy 97.97\n",
            "\t Validation loss 0.00025, Validation accuracy 96.27\n",
            "-----------------------------------------------------\n",
            "Epoch: 8\n",
            "\t Training loss 0.00023, Training accuracy 98.23\n",
            "\t Validation loss 0.00024, Validation accuracy 96.39\n",
            "-----------------------------------------------------\n",
            "Epoch: 9\n",
            "\t Training loss 0.00019, Training accuracy 98.48\n",
            "\t Validation loss 0.00023, Validation accuracy 96.58\n",
            "-----------------------------------------------------\n",
            "Epoch: 10\n",
            "\t Training loss 0.00016, Training accuracy 98.82\n",
            "\t Validation loss 0.00022, Validation accuracy 96.75\n",
            "-----------------------------------------------------\n",
            "Epoch: 11\n",
            "\t Training loss 0.00014, Training accuracy 99.07\n",
            "\t Validation loss 0.00023, Validation accuracy 96.65\n",
            "-----------------------------------------------------\n",
            "Epoch: 12\n",
            "\t Training loss 0.00012, Training accuracy 99.24\n",
            "\t Validation loss 0.00022, Validation accuracy 96.88\n",
            "-----------------------------------------------------\n",
            "Epoch: 13\n",
            "\t Training loss 0.00010, Training accuracy 99.42\n",
            "\t Validation loss 0.00023, Validation accuracy 96.93\n",
            "-----------------------------------------------------\n",
            "Epoch: 14\n",
            "\t Training loss 0.00009, Training accuracy 99.51\n",
            "\t Validation loss 0.00024, Validation accuracy 96.81\n",
            "-----------------------------------------------------\n",
            "Epoch: 15\n",
            "\t Training loss 0.00009, Training accuracy 99.53\n",
            "\t Validation loss 0.00023, Validation accuracy 96.93\n",
            "-----------------------------------------------------\n",
            "Epoch: 16\n",
            "\t Training loss 0.00007, Training accuracy 99.71\n",
            "\t Validation loss 0.00024, Validation accuracy 96.83\n",
            "-----------------------------------------------------\n",
            "Epoch: 17\n",
            "\t Training loss 0.00006, Training accuracy 99.77\n",
            "\t Validation loss 0.00023, Validation accuracy 96.94\n",
            "-----------------------------------------------------\n",
            "Epoch: 18\n",
            "\t Training loss 0.00005, Training accuracy 99.86\n",
            "\t Validation loss 0.00023, Validation accuracy 97.08\n",
            "-----------------------------------------------------\n",
            "Epoch: 19\n",
            "\t Training loss 0.00004, Training accuracy 99.88\n",
            "\t Validation loss 0.00023, Validation accuracy 97.14\n",
            "-----------------------------------------------------\n",
            "Epoch: 20\n",
            "\t Training loss 0.00004, Training accuracy 99.92\n",
            "\t Validation loss 0.00023, Validation accuracy 97.00\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00003, Training accuracy 99.94\n",
            "\t Validation loss 0.00023, Validation accuracy 97.00\n",
            "\t Test loss 0.00019, Test accuracy 97.32\n",
            "-----------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}