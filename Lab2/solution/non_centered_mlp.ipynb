{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "non_centered_mlp.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mancinimassimiliano/DeepLearningLab/blob/master/Lab2/solution/non_centered_mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "p18hTNZDSBF_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Introduction\n",
        "\n",
        "In this notebook we will see the effect of translating MNIST digits on the classification accuracy when a MLP is used for training. \n",
        "\n",
        "In details, we will shift the digits horizontally and/or vertically in an image and monitor their impact on training and the final performance of the network. For eg. if we shift a digit \"five\" to one corner of the image, can the MLP network still identify it as a digit \"five\"?"
      ]
    },
    {
      "metadata": {
        "id": "zxrTp9ojAbAO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as T\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Libraries needed for visualization purposes\n",
        "from tensorboardcolab import TensorBoardColab\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Instantiate Tensorboard visualizer\n",
        "# TODO "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "okj39hMhTYcp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the 'translation' transformation function that generates the *custom* translated MNIST dataset.\n",
        "\n",
        "Useful links:\n",
        "\n",
        "[`torch.randint`](https://pytorch.org/docs/stable/torch.html#torch.randint) for generating random integers.\n",
        "\n",
        "[`torch.nn.functional.pad`](https://pytorch.org/docs/stable/nn.html#torch.nn.functional.pad) for padding zeros to images.\n",
        "\n",
        "[`torchvision.transforms.Lambda`](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Lambda) for applying user-defined lambda as a function.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "NBE-qnZ4FPuD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This function will shift digits horizontally and/or vertically, i.e., translation transformation\n",
        "def apply_translation_transform():\n",
        "  def padding(x):\n",
        "      # this function will add random zero padding to the MNIST digits\n",
        "      pad_size = 28\n",
        "      left_padding = # TODO\n",
        "      top_padding = #TODO\n",
        "      return # TODO\n",
        "  \n",
        "  # We would apply a list of transformations, sequentially\n",
        "  translation_transform = list()\n",
        "  translation_transform.append(T.ToTensor())\n",
        "  # TODO for padding zero values around digits\n",
        "  # TODO repeat channels just for the sake of matplotlib visualization\n",
        "  # TODO combine all the desired transformations\n",
        "  \n",
        "  return translation_transform"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zd1WtZ42T4mw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Function for Dataset Visualization\n",
        "This function loads a batch of images which have been translated horizontally or vertically"
      ]
    },
    {
      "metadata": {
        "id": "EGF4hcUGAsOB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data_for_visualization(batch_size, translate=False): \n",
        "  \n",
        "  if not translate:\n",
        "    # Image transformation that appends 14 pixels on each side of a digit\n",
        "    transform = list()\n",
        "    transform.append(T.ToTensor())\n",
        "    # TODO for padding zero values uniformly on all sides, so that the digits are still at the center\n",
        "    # TODO combine all the desired transformations\n",
        "  else:\n",
        "    # Applies random translations to images\n",
        "    transform = apply_translation_transform()\n",
        "\n",
        "  # Load data\n",
        "  full_training_data = torchvision.datasets.MNIST('./data', train=True, transform=transform, download=True)\n",
        "\n",
        "  # Initialize dataloaders\n",
        "  train_loader = torch.utils.data.DataLoader(full_training_data, batch_size, shuffle=True)\n",
        "  \n",
        "  return train_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ItRVKFTGUV4Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize both translated and non-translated MNIST digits"
      ]
    },
    {
      "metadata": {
        "id": "qNHdwGRqGN2T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Get a bunch of training images for visualization\n",
        "train_loader = get_data_for_visualization(256, translate=False)\n",
        "train_loader_translated = get_data_for_visualization(256, translate=True)\n",
        "\n",
        "train_iter, train_iter_translated = iter(train_loader), iter(train_loader_translated)\n",
        "\n",
        "# Get a single batch of data and labels of centered digits\n",
        "data, labels = next(train_iter)\n",
        "\n",
        "# Get a single batch of data and labels of translated digits\n",
        "data_translated, labels_translated = next(train_iter_translated)\n",
        "\n",
        "# the label of the digit you want to visualize\n",
        "digit_label = 8\n",
        "\n",
        "# get first 9 indices of the chosen digit for non-translated digits\n",
        "get_idx = (labels == digit_label).nonzero().squeeze(-1)[0:9]\n",
        "\n",
        "# get first 9 indices of the chosen digit for translated digits\n",
        "get_idx_translated = (labels_translated == digit_label).nonzero().squeeze(-1)[0:9]\n",
        "\n",
        "# this gets the data and labels for the chosen certain digit\n",
        "get_data, get_labels = data[get_idx, :, :, :], labels[get_idx]\n",
        "get_data_translated, get_labels_translated = data_translated[get_idx_translated, :, :, :], \\\n",
        "                                             labels_translated[get_idx_translated]\n",
        "\n",
        "\n",
        "# visualize the plots inline, both for translated and non-translated\n",
        "# first the non-translated digits\n",
        "display_grid = torchvision.utils.make_grid(get_data, nrow=3, padding=2, pad_value=1)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(display_grid.numpy().transpose(1,2,0))\n",
        "plt.axis('off')\n",
        "plt.title('Centered Digits')\n",
        "\n",
        "# the the translated digits\n",
        "display_grid_translated = torchvision.utils.make_grid(get_data_translated, nrow=3, padding=2, pad_value=1)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(display_grid_translated.numpy().transpose(1,2,0))\n",
        "plt.axis('off')\n",
        "plt.title('Translated Digits')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bAddchT2sTRW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define our first MLP"
      ]
    },
    {
      "metadata": {
        "id": "bXhpFqgiQtzn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Our previous MLP\n",
        "class MyFirstNetwork(torch.nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "    super(MyFirstNetwork, self).__init__()\n",
        "    \n",
        "    self.input_to_hidden = torch.nn.Linear(input_dim, hidden_dim)\n",
        "    self.hidden_to_output = torch.nn.Linear(hidden_dim, output_dim)\n",
        "    self.activation = torch.nn.Sigmoid()\n",
        "    \n",
        "    self.input_to_hidden.bias.data.fill_(0.)\n",
        "    self.hidden_to_output.bias.data.fill_(0.)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(x.shape[0],-1)\n",
        "    x = self.input_to_hidden(x)\n",
        "    x = self.activation(x)\n",
        "    x = self.hidden_to_output(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UW1Q7PcRsiEh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define cost function"
      ]
    },
    {
      "metadata": {
        "id": "nX97K__NQ4v1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_cost_function():\n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k43yfdpysvHA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the optimizer"
      ]
    },
    {
      "metadata": {
        "id": "GA5qoK-SQ8D1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_optimizer(net, lr, wd, momentum):\n",
        "  optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n",
        "  return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LGOnEYS4tFfD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train and test functions"
      ]
    },
    {
      "metadata": {
        "id": "Ob9KExsttEGR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(net, data_loader, cost_function, device='cuda:0'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  net.eval() # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      # Load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "        \n",
        "      # Forward pass\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      # Apply the loss\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # Better print something\n",
        "      samples+=inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = outputs.max(1)\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100\n",
        "\n",
        "def train(net,data_loader,optimizer,cost_function, device='cuda:0'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  \n",
        "  net.train() # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "    # Load data into GPU\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "      \n",
        "    # Forward pass\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    # Apply the loss\n",
        "    loss = cost_function(outputs,targets)\n",
        "\n",
        "    # Reset the optimizer\n",
        "      \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Better print something, no?\n",
        "    samples+=inputs.shape[0]\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OR5rOezcTu8Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the function that fetches a data loader that is used during iterative training."
      ]
    },
    {
      "metadata": {
        "id": "yIlQY7ROt4mc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data(batch_size, test_batch_size=128, translate=False): \n",
        "  \n",
        "  if translate:\n",
        "    def padding(x):\n",
        "        pad_size = 28\n",
        "        left_padding = torch.randint(low=0, high=pad_size, size=(1,))\n",
        "        top_padding = torch.randint(low=0, high=pad_size, size=(1,))\n",
        "        return F.pad(x, (left_padding, \n",
        "                         pad_size - left_padding, \n",
        "                         top_padding, \n",
        "                         pad_size - top_padding), \"constant\", 0)\n",
        "\n",
        "    transform = list()\n",
        "    transform.append(T.ToTensor())\n",
        "    transform.append(T.Lambda(lambda x: padding(x)))\n",
        "    transform = T.Compose(transform)\n",
        "  else:\n",
        "    transform = list()\n",
        "    transform.append(T.ToTensor())\n",
        "    transform.append(T.Lambda(lambda x: F.pad(x, (14, 14, 14, 14), \"constant\", 0)))\n",
        "    transform = T.Compose(transform)\n",
        "    \n",
        "  # Load data\n",
        "  full_training_data = torchvision.datasets.MNIST('./data', train=True, transform = transform, download=True) \n",
        "  test_data = torchvision.datasets.MNIST('./data', train=False, transform=transform, download=True) \n",
        "  \n",
        "  # Create train and validation splits\n",
        "  num_samples = len(full_training_data)\n",
        "  training_samples = int(num_samples*0.5+1)\n",
        "  validation_samples = num_samples - training_samples\n",
        "\n",
        "  training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\n",
        "\n",
        "  # Initialize dataloaders\n",
        "  train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True)\n",
        "  val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False)\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False)\n",
        "  \n",
        "  return train_loader, val_loader, test_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rvWju1A5tUtm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the wrapper function that wraps everything for training a MLP"
      ]
    },
    {
      "metadata": {
        "id": "5N1uPayYtsG3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Input arguments:\n",
        "  batch_size: The size of a mini-batch that is used for training\n",
        "  input_dim: Flatenned size of the input image vector\n",
        "  hidden_dim: Number of hidden neurons in the network\n",
        "  output_dim: The number of output neurons\n",
        "  device: GPU where you want to train your network\n",
        "  learning_rate: Learning rate for the optimizer\n",
        "  weight_decay: Weight decay co-efficient for regularization of weights\n",
        "  momentum: Momentum for SGD optimizer\n",
        "  epochs: Number of epochs for training the network\n",
        "  translate: Whether to translate the images that are fed to the network\n",
        "  visualization_name: Name of the graph for visualizing in tensorboards\n",
        "                      (Always remember to use an unique visualization_name\n",
        "                       for each training. Otherwise it will mess up the visualization.)\n",
        "'''\n",
        "\n",
        "def main_MLP(batch_size=64, input_dim=56*56, hidden_dim=100, output_dim=10, device='cuda:0', \n",
        "             learning_rate=0.01, weight_decay=0.000001, momentum=0.9, epochs=50, \n",
        "             translate=False, visualization_name='centered'):\n",
        "  \n",
        "  train_loader, val_loader, test_loader = get_data(batch_size=batch_size, translate=translate)\n",
        "  \n",
        "  device = torch.device(device)\n",
        "  \n",
        "  net = MyFirstNetwork(input_dim, hidden_dim, output_dim).to(device)\n",
        "  \n",
        "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "  \n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  print('Before training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "  \n",
        "  # Add values to plots\n",
        "  # TODO \n",
        "  \n",
        "  # Update plots \n",
        "  # TODO\n",
        "\n",
        "  for e in range(epochs):\n",
        "    train_loss, train_accuracy = train(net, train_loader, optimizer, cost_function)\n",
        "    val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "    \n",
        "    # Add values to plots\n",
        "    # TODO \n",
        "    \n",
        "    # Update plots \n",
        "    # TODO\n",
        "\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a2t1llmmvsg9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Train your first MLP with MNIST digits that are centered (i.e. not translated). "
      ]
    },
    {
      "metadata": {
        "id": "sQn6IAQdvp_I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "main_MLP(translate=False, visualization_name='centered')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZRNt0-WfyOPu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Train your first MLP with MNIST digits that are not centered (i.e. translated digits). Can we match our previous accuracy with translated inputs?"
      ]
    },
    {
      "metadata": {
        "id": "5rRPsoTPAlSI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# set translate to True\n",
        "main_MLP(translate=True, visualization_name='translated')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}