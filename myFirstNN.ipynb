{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mancinimassimiliano/DeepLearningLab/blob/master/myFirstNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "2DtAgGFhCE_s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Welcome to the first session of the Deep Learning Lab! \n",
        "---\n",
        "## Today we will learn how to set up and train a neural network in PyTorch\n",
        "###**TASK:  Train a multi-layer perceptron (MLP) for digit recognition (on MNIST)**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Let us start with the imports. We need `torch` and  `torchvision`. The first contains all the tools we need for training a network, while the second contains practical shortcuts for datasets and other stuffs (e.g. pretrained models).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "JAGJ-DgfCDhV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "Dqz7LBxwDbru",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch, torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hxhHgihSGNAa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "KbcIR4gIDlXX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now that we have the tools, let us define a function which allows us to load the MNIST data. For doing so, we need a dataset (`torch.utils.data.Dataset`) and a loader (`torch.utils.data.Dataloader`), allowing us to llop over the dataset. For MNIST PyTorch already contains a dataset definition, which you can find [here](https://pytorch.org/docs/stable/torchvision/datasets.html#mnist). For what concerns the dataloader, default ones can be found [here](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader). We must create train, validation and test set and a loader for each of them."
      ]
    },
    {
      "metadata": {
        "id": "_OLqgNgCGO9F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data(batch_size, test_batch_size=256): \n",
        "  # This function is needed to convert the PIL images to Tensors\n",
        "  transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
        "\n",
        "  # Load data\n",
        "  full_training_data = # TODO \n",
        "  test_data = # TODO\n",
        "  \n",
        "\n",
        "  # Create train and validation splits\n",
        "  num_samples = len(full_training_data)\n",
        "  training_samples = int(num_samples*0.5+1)\n",
        "  validation_samples = num_samples - training_samples\n",
        "\n",
        "  training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\n",
        "\n",
        "  # Initialize dataloaders\n",
        "  train_loader = # TODO\n",
        "  val_loader = # TODO\n",
        "  test_loader = # TODO\n",
        "  \n",
        "  return train_loader, val_loader, test_loader\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "40GUUErvGgfv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now that we have the data, what we need is a network. For now let us instantiate an MLP with 2 fully-connected layers (input-to-hidden and hidden-to-output). The input size is 28x28x1 (greyscale images) and the output is 10, as the number of digits.  The fully-connected layers are defined as `torch.nn.Linear`.  Between the layers we must put a non-linear activation. For now let us use a sigmoid (`torch.nn.Sigmoid`). For other layers and activation functions please have a look at the [doc](https://pytorch.org/docs/stable/nn.html). Do not forget that a network must extend a `torch.nn.Module`."
      ]
    },
    {
      "metadata": {
        "id": "b4MAGF_LKEQ5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Our network\n",
        "class MyFirstNetwork(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "    super(MyFirstNetwork, self).__init__()\n",
        "    # TODO: Initialize layers\n",
        "\n",
        "  def forward(self, x):\n",
        "    # TODO: Define forward pass\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TZMmfyFNKWGY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For training the network, we obviously need a loss function. The task is classification with multiple classes, thus a proper loss could be a cross-entropy with softmax. We can again use `torch.nn` which contains several losses, among which `torch.nn.CrossEntropyLoss`. Notice that this loss already contains the softmax activation, thus we do not need to apply the softmax to the output of our network."
      ]
    },
    {
      "metadata": {
        "id": "SFnlG_hCLpqJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_cost_function():\n",
        "  # TODO: Initialize the loss function (hint: have a look at torch.nn)\n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cJRtK9IcLsNB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we must devise a way to update the parameters of our network. This can be easily held out by having a look at [`torch.optim`](https://pytorch.org/docs/stable/optim.html) which contains a large variety of optimizers."
      ]
    },
    {
      "metadata": {
        "id": "MeW_KX80Nwrr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_optimizer(net, lr, wd, momentum):\n",
        "  # TODO: Create and optimizer (hint: have a look at torch.optim; you can retrieve networks params through net.parameters())\n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VpZ8snpiYRFK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We are ready to merge everything by creating a training and test functions. Both of them must:\n",
        "\n",
        "1.   Loop over the data (exploiting the dataloader, which is just an iterator)\n",
        "2.   Forward the data through the network\n",
        "3.  Comparing the output with the target labels for computing either the loss (train), the accuracy (test) or both.\n",
        "\n",
        "Additionally, during training we must:\n",
        "\n",
        "\n",
        "1.   Compute the gradient with the backward pass (`loss.backward()`)\n",
        "2.   Using the optimizer to update the weights (`optimizer.step()`)\n",
        "3.   Cleaning the gradient of the weights in order to not accumulating it (`optimizer.zero_grad()`)\n",
        "\n",
        "With these steps in mind, we are ready to define everything.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "jHrTkmnWaZnp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(net, data_loader, cost_function, device='cuda'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  net.eval() # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      # Load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "        \n",
        "      # Forward pass\n",
        "      outputs = # TODO\n",
        "\n",
        "      # Apply the loss\n",
        "      loss = # TODO\n",
        "\n",
        "      # Better print something\n",
        "      samples+=inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = outputs.max(1)\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(net,data_loader,optimizer,cost_function, device='cuda'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  \n",
        "  net.train() # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "    # Load data into GPU\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "      \n",
        "    # Forward pass\n",
        "    outputs = # TODO\n",
        "\n",
        "    # Apply the loss\n",
        "    loss = # TODO\n",
        "\n",
        "    # Reset the optimizer\n",
        "      \n",
        "    # Backward pass\n",
        "      \n",
        "    # Update parameters\n",
        "\n",
        "    # Better print something, no?\n",
        "    samples+=inputs.shape[0]\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6QOcUCgwbers",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, we need a main function which initializes everything + the needed hyperparameters and loops over multiple epochs (printing the results)."
      ]
    },
    {
      "metadata": {
        "id": "x7eUGpq2b5YX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main(batch_size=128, input_dim=28*28, hidden_dim=100, output_dim=10, device='cuda:0', learning_rate=0.01, weight_decay=0.000001, momentum=0.9, epochs=10):\n",
        "  train_loader, val_loader, test_loader = get_data(batch_size)\n",
        "  net = MyFirstNetwork(input_dim, hidden_dim, output_dim).to(device)\n",
        "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  print('Before training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  for e in range(epochs):\n",
        "    train_loss, train_accuracy = train(net, train_loader, optimizer, cost_function)\n",
        "    val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7YnZVLrLcnBQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And now let the magic happen :)"
      ]
    },
    {
      "metadata": {
        "id": "O4xHceYqcljf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}